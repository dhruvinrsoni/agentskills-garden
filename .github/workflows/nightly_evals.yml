# Nightly Evaluations Workflow
# Runs LLM-as-judge evaluations on golden datasets

name: Nightly Evaluations

on:
  schedule:
    # Run at 2 AM UTC every day
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      dataset:
        description: 'Specific dataset to evaluate (leave empty for all)'
        required: false
        type: string
      skill:
        description: 'Specific skill to evaluate (leave empty for all)'
        required: false
        type: string

env:
  PYTHON_VERSION: '3.11'

jobs:
  discover-skills:
    name: Discover Skills
    runs-on: ubuntu-latest
    outputs:
      skills: ${{ steps.find-skills.outputs.skills }}
    steps:
      - uses: actions/checkout@v4

      - name: Find all skills
        id: find-skills
        run: |
          # Find all skill directories that have test files
          skills=$(find skills -name "schema.json" -exec dirname {} \; | jq -R -s -c 'split("\n") | map(select(length > 0))')
          echo "skills=$skills" >> $GITHUB_OUTPUT

  run-evaluations:
    name: Evaluate Skills
    needs: discover-skills
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        skill: ${{ fromJson(needs.discover-skills.outputs.skills) }}
    
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install base dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r infra/requirements.txt

      - name: Install skill dependencies
        run: |
          if [ -f "${{ matrix.skill }}/requirements.txt" ]; then
            pip install -r "${{ matrix.skill }}/requirements.txt"
          fi

      - name: Start skill service
        run: |
          cd "${{ matrix.skill }}"
          uvicorn main:app --host 0.0.0.0 --port 8000 &
          sleep 5  # Wait for service to start

      - name: Run health check
        run: |
          curl --fail http://localhost:8000/health || exit 1

      - name: Run unit tests
        run: |
          cd "${{ matrix.skill }}"
          pytest test_main.py -v --tb=short

      - name: Run evaluation judge
        if: hashFiles('evals/golden_dataset/**/*.json') != ''
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          skill_name=$(basename "${{ matrix.skill }}")
          python evals/judge.py \
            --skill "$skill_name" \
            --dataset "evals/golden_dataset/" \
            --output "eval_results_${skill_name}.json" \
            --endpoint "http://localhost:8000/execute"

      - name: Upload evaluation results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-results-${{ hashFiles(matrix.skill) }}
          path: eval_results_*.json
          retention-days: 30

  aggregate-results:
    name: Aggregate Results
    needs: run-evaluations
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: results/
          pattern: eval-results-*
          merge-multiple: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Generate summary report
        run: |
          python << 'EOF'
          import json
          import os
          from pathlib import Path
          from datetime import datetime

          results_dir = Path("results")
          summary = {
              "timestamp": datetime.utcnow().isoformat(),
              "skills": {},
              "totals": {"passed": 0, "failed": 0, "skipped": 0}
          }

          for result_file in results_dir.glob("eval_results_*.json"):
              try:
                  with open(result_file) as f:
                      data = json.load(f)
                  skill_name = result_file.stem.replace("eval_results_", "")
                  summary["skills"][skill_name] = data
                  summary["totals"]["passed"] += data.get("passed", 0)
                  summary["totals"]["failed"] += data.get("failed", 0)
              except Exception as e:
                  print(f"Error processing {result_file}: {e}")

          # Write summary
          with open("evaluation_summary.json", "w") as f:
              json.dump(summary, f, indent=2)

          # Generate markdown report
          report = f"""# Nightly Evaluation Report
          
          **Date**: {summary['timestamp']}
          
          ## Summary
          
          | Metric | Count |
          |--------|-------|
          | Passed | {summary['totals']['passed']} |
          | Failed | {summary['totals']['failed']} |
          | Skipped | {summary['totals']['skipped']} |
          
          ## Skills Evaluated
          
          """
          for skill, data in summary["skills"].items():
              report += f"- **{skill}**: {data.get('passed', 'N/A')} passed\n"

          with open("EVALUATION_REPORT.md", "w") as f:
              f.write(report)

          print("Summary generated successfully")
          EOF

      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-summary
          path: |
            evaluation_summary.json
            EVALUATION_REPORT.md
          retention-days: 90

      - name: Post summary to job
        run: |
          if [ -f "EVALUATION_REPORT.md" ]; then
            cat EVALUATION_REPORT.md >> $GITHUB_STEP_SUMMARY
          fi

  notify-on-failure:
    name: Notify on Failure
    needs: [run-evaluations, aggregate-results]
    runs-on: ubuntu-latest
    if: failure()
    
    steps:
      - name: Create issue on failure
        uses: actions/github-script@v7
        with:
          script: |
            const date = new Date().toISOString().split('T')[0];
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Nightly Evaluation Failed - ${date}`,
              body: `The nightly evaluation workflow failed on ${date}.\n\nSee [workflow run](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}) for details.`,
              labels: ['evaluation', 'automated']
            });
